### 텍스트 전처리

____



#### 1. 토큰화(tokenization)

= 주어진 corpus에서 token이라 불리는 단위로 나누는 작업



1. **단어 토큰화**

   - 단어(word): 상황에 따라 단어, 단어구, 의미를 갖는 문자열 등
   - 정제(cleaning): 구두점, 특수문자 등을 모두 제거

   

2. **토큰화에서 고려해야할 사항**

   - 예상치 못한 경우에 토큰화의 기준을 선택해야 하는 경우가 발생
     - *ex) Don't => Don't? Don, t? Do, n't?*

   - 구두점이나 특수 문자를 단순 제외해서는 안 됨

     - *ex) 구두점조차도 하나의 토큰으로 분류하는 경우, 단어 자체에서 구두점을 포함하고 있는 경우*

   - 줄임말과 단어 내에 띄어쓰기가 있는 경우

     - *ex) we're, New York*

     

     #### => Tokenization Tool마다 토큰화 기준과 규칙이 모두 다름

     #### => 사용하고자 하는 용도에 어떤 Tool이 가장 적절한지 판단 필요

     

3. **문장 토큰화**

   - !, ? 등은 꽤 명확한 문장의 구분자 역할을 하지만 . 은 문장의 끝이 아닌 경우에도 등장할 수 있음

   

4. **이진 분류기(Binary Classifier)**

   - 온점의 처리를 위해서 입력에 따라 두 개의 클래스로 분류

     (a) 온점이 단어의 일부분일 경우, 즉 온점이 약어로 쓰이는 경우

     ​	=> 어떤 온점이 주로 약어로 쓰이는 지 알아야 하기 때문에 약어(abbreviation) 사전이 유용함

     (b) 온점이 문장의 구분자의 경우

     

5. **한국어에서 토큰화의 어려움**

   - 교착어: 한국어는 어절이 독립적인 단어로 구성되는 것이 아니라 조사 등의 무언가가 붙어있는 경우가 많음
   - 형태소: 뜻을 가진 가장 작은 말의 단위
     - 자립 형태소: 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소, 그 자체로 단어가 됨
     - 의존 형태소: 다른 형태소와 결합하여 사용되는 형태소

   - 한국어에서 영어 단어 토큰화와 유사한 형태를 얻으려면 어절 토큰화가 아닌 형태소 토큰화를 수행해야 함

   

6. (corpus의 성격에 따라) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않음

   - 이유?

     => 한국어의 경우 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있음(띄어쓰기는 1933년에야 보편화)

     

7. 품사 태깅(Part-of-speech Tagging)

   - 단어의 표기는 같지만 품사에 따라서 단어의 의미가 달라지는 경우
     - *ex1) fly = 날다(동사), 파리(명사)*
     - *ex2) 못 = 망치를 사용해서 목재 따위를 고정하는 물건(명사), 어떤 동작을 할 수 없음(부사)*





____



#### 2. 정제(Cleaning), 정규화(Normalization)

- 정제: 갖고 있는 corpus로부터 노이즈 데이터를 제거한다

- 정규화: 표현 방법이 다른 단어들을 통합시켜서 같은 단어로 만든다

  #### => corpus의 복잡성을 줄인다



1. **규칙에 기반한 표기가 다른 단어들의 통합**

   *ex) USA & US, uh-huh & uhhuh*

   - 어간 추출(stemming)

   - 표제어 추출(lemmatization)

     

2. **대/소문자 통합**

   - 대부분의 글이 소문자로 작성되기 때문에 대/소문자 통합 작업은 일반적으로 대문자를 소문자로 변환하는  방식으로 이루어짐

   - 예외 존재(*US & us, General Motors*)

     

3. **불필요한 단어의 제거**

   - 노이즈 데이터: 자연어가 아니면서 아무 의미도 갖지 않는 글자 & 분석하고자 하는 목적에 맞지 않는 불필요 단어들

     (1) 등장 빈도가 적은 단어

     (2) 길이가 짧은 단어 => 한국어는 해당 x

     

4. **정규 표현식(Regular Expression)**

   - 얻어낸 corpus에서 노이즈 데이터의 특징을 잡아낼 수 있다면 정규 표현식을 통해서 이를 잡아낼 수 있음





____



#### 3. 표제어 추출(Lemmatization), 어간 추출(Stemming)

- 형태학적 파싱(형태소 단위)

  - 어간(stem): 단어의 의미를 담고 있는 핵심 부분

  - 접사(affix): 단어에 추가적인 의미를 주는 부분



1. **표제어 추출**

   - 기본 사전형 단어

   - *ex) am, are, is => be*

   - 표제어 추출의 경우 문맥을 고려하며 과정을 거쳐도 해당 단어의 품사 정보를 보존

     

2. **어간 추출**

   - 정해진 규칙만 보고 단어의 어미를 자르는 경우가 많아 제대로 된 일반화를 수행하지 못할 수 있음
   - *ex) organization => organ, having => hav*



3. **한국어에서의 어간 추출**

   - 용언: 동사, 형용사

   - 활용(conjugation)

     - 용언의 어간이 어미를 가지는 일
     - 어간(stem): 용언을 활용할 때 원칙적으로 모양이 변하지 않는 부분
     - 어미(ending): 용언의 어간 뒤에 붙어서 변하는 부분, 문법적 기능 수행

     

     - **규칙 활용: 어간이 어미를 취할 때 어간의 모습이 일정한 경우**

       - *잡 (어간) + 다 (어미)*
       - *잡 (어간) + 은 (어미)*

       => <u>규칙 기반으로 어미를 단순히 분리해주면 됨</u>

       

     - **불규칙 활용: 어간이 어미를 취할 때 어간의 모습이 바뀌거나 취하는 어미가 특수한 어미일 경우**

       - *듣 (어간) + 다 (어미)*
       - *들 (어간) + 은 (어미)*

       => <u>단순한 분리만으로 어간 추출이 되지 않기 때문에 더 복잡한 규칙 필요</u>





____



#### 4. 불용어(Stopword)

= 조사, 접미사 등 실제 의미 분석에 기여하는 바가 매우 작거나 없는 단어



1. 한국어에서 불용어 제거하기
   - 기본적으로는 토큰화 후에 조사, 접속사 등을 제거
     + 불용어를 직접 정의해서 명사, 형용사 등에도 적용 가능





____



**\* 정수 인코딩(Integer Encoding)**

- 각 단어를 고유한 정수(index)에 맵핑하는 작업



**\* 패딩(Padding)**

- 길이가 전부 동일한 문서들에 대해서는 하나의 행렬로 보고 묶어서 처리 가능

- 병렬 연산을 위해서 여러 문장의 길이를 임의로 동일하게 맞추는 작업





____



#### 5. One-Hot Encoding

- **vocabulary (단어 집합, 사전)**
  - 서로 다른 단어들의 집합
  - 한 단어의 변형 형태도 다른 단어로 간주

- 단어 집합의 크기를 벡터의 차원으로 하고 표현하고 싶은 단어의 인덱스에 1, 다른 인덱스에 0을 부여하는 단어의 벡터 표현 방식

- 한계

  - 단어의 개수가 늘어날수록 벡터의 차원이 늘어남 = 벡터를 저장하기 위해 필요한 공간이 늘어남

  - 단어의 유사도를 표현하지 못함

    

    #### => 단어의 잠재 의미를 반영하여 다차원 공간에 벡터화 하는 기법 (카운트 기반: LSA, HAL / 예측 기반: NNLM,  RNNLM, Word2Vec, FastText)





____

*출처: https://wikidocs.net/21694 - 딥러닝을 이용한 자연어 처리 입문*