#### 한국어 임베딩 예제 코드

___

- p.81

  ```python
  from gensim.corpora import WikiCorpus
  from gensim.utils import to_unicode
  from gensim.corpora.wikicorpus import tokenize  //*추가해줘야함
  
  in_f = "/notebooks/embedding/data/raw/kowiki-latest-pages-articles.xml.bz2"
  out_f = "/notebooks/embedding/data/processed/processed_wiki_ko.txt"
  output = open(out_f, 'w')
  wiki = WikiCorpus(in_f, tokenizer_func=tokenize)
  i = 0
  for text in wiki.get_texts():
          output.write(bytes(' '.join(text), 'utf-8').decode('utf-8') + '\n')
          i = i + 1
          if (i % 10000 == 0):
                  print('Processed ' + str(i) + ' articles')
  output.close()
  print('Processing complete!')
  ```



- p.96

  ```python
  from konlpy.tag import Okt, Komoran, Mecab, Hannanum, Kkma
  
  def get_tokenizer(tokenizer_name):
          if tokenizer_name == "komoran":
                  tokenizer = Komoran()
          elif tokenizer_name == "okt":
                  tokenizer = Okt()
          elif tokenizer_name == "mecab":
                  tokenizer = Mecab()
          elif tokenizer_name == "hannanum":
                  tokenizer = Hannanum()
          elif tokenizer_name == "kkma":
                  tokenizer = Kkma()
          else:
                  tokenizer = Mecab()
          return tokenizer
  
  
  print("input tokenizer: komoran, okt, mecab, hannanum, kkma")
  tokenizer = input()
  print("input text: ")
  text = input()
  print(get_tokenizer(tokenizer).morphs(text))
  print(get_tokenizer(tokenizer).pos(text))
  ```



- p.97

  ```python
  from khaiii import KhaiiiApi
  
  tokenizer = KhaiiiApi()
  data = tokenizer.analyze("아버지가방에들어가신다")
  tokens = []
  for word in data:
          tokens.extend([str(m).split("/")[0] for m in word.morphs])
  
  print(tokens)
  ```



- 